<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tommy Back]]></title>
  <link href="http://murrekatt.github.com/atom.xml" rel="self"/>
  <link href="http://murrekatt.github.com/"/>
  <updated>2015-09-08T13:19:26+02:00</updated>
  <id>http://murrekatt.github.com/</id>
  <author>
    <name><![CDATA[Tommy Back]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS S3 Pre-signed POST Example on Google App Engine]]></title>
    <link href="http://murrekatt.github.com/blog/2015/09/08/aws-s3-pre-signed-post-example-on-google-app-engine/"/>
    <updated>2015-09-08T12:49:00+02:00</updated>
    <id>http://murrekatt.github.com/blog/2015/09/08/aws-s3-pre-signed-post-example-on-google-app-engine</id>
    <content type="html"><![CDATA[<p>As a follow-up post to my post about <a href="tommyback.com/blog/2015/09/08/offloading-static-content-for-your-web-service/">Offloading Your Static Content for Your
Web Service</a>
I have created a small example <a href="https://cloud.google.com/appengine/docs">Google App Engine</a>
project to demonstrate the HTTP POST from the browser.</p>

<p>You can find the <a href="https://github.com/murrekatt/go-aws-s3-presigned-post-app-engine">example on Github here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Offloading Static Content for Your Web Service]]></title>
    <link href="http://murrekatt.github.com/blog/2015/09/08/offloading-static-content-for-your-web-service/"/>
    <updated>2015-09-08T11:21:00+02:00</updated>
    <id>http://murrekatt.github.com/blog/2015/09/08/offloading-static-content-for-your-web-service</id>
    <content type="html"><![CDATA[<p>For web services with lot of static content that is user generated or user-
consumed content like for instance sound or video, it would normally be sent to
the backend and thus processed there. This means you need to pay bandwidth and
processing of that data. It does not matter if you are hosted on a PaaS or run
your own server infrastructure, you still need to pay to processthe data. You
alos need to deal with scaling the delivery (see Content Delivery Networks).</p>

<p>Unless you really need to get the data to your service endpoint there is a much
better way to offload this and that is to use Amazon S3 or something similar
like Google Blob Store. These can function like a CDN and can scale globally
across datacenters and continents as needed by your service.</p>

<p>Depending on the situation, e.g. if the web service is a public internet
service, a company-internal service or who generates the content, who consumes
it and how is it shared one would set up the overall system slightly different.</p>

<p>For internet services, the use case is many times that users generate content
and upload that as part of the service for themselves, for sharing or both. The
content can be private, shared or public depending on. Let’s now look at the
situation with a public internet service like Youtube where you manage videos
that users upload.</p>

<h2>Use Case</h2>

<p>In a scenario like Youtube where you have a site where users can publicly
search, play and share videos you basically want a storage that is readable by
anyone and writable as the service allows. Here read means search and download,
and, writable means create. Direct deletion of files should not be allowed as
this is not expensive in terms of bandwidth or processing and the system might
still want to keep the contents and just mark it not available.</p>

<h3>Overview</h3>

<p>What we want to set up is a AWS S3 bucket with public read and nothing else.
I.e. anyone can download given a URL that points of an object in the bucket.
Then we want to let the owner of the bucket, i.e. the controlling service
hand out permissions to upload as users want to add new videos to the service.
This means the service is aware of uploads and can control them.</p>

<p>The permissions are essentially signed URLs or policies that describe what is
allowed. E.g. user A is allowed to upload a file with a specific key
(destination in the bucket), file size and within the next hour. After that
the permission is no longer valid.</p>

<p>Let’s see how to set this up with AWS S3.</p>

<h3>Bucket</h3>

<p>The bucket needs to be set up with public read and nothing else. This is best
done with a bucket policy like so:</p>

<pre><code>{
  "Version": "2012-10-17",
  "Id": "Policy1234567890",
  "Statement": [{
    "Sid": "Stmt1234567890",
    "Effect": "Allow",
    "Principal": {
      "AWS": "*"
    },
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::bucketnamehere/*"
  }]
}
</code></pre>

<p>No other permissions should be per default except for the owner (Me) there is a
ACL (Access Control List) that gives all rights. Those rights are later given
on a case by case basis to users that want to upload content.</p>

<p>Let’s look at how to give permissions.</p>

<h3>Pre-signed URLs</h3>

<p>The simplest way to give permission to upload is to generate a pre-signed URL to
be used with an HTTP PUT method. This means all necessary permission information
is part of the URL as parameters.</p>

<p>Please note that browsers do not support HTTP PUT so one must either do it with
Javascript or do the second option which is HTTP POST.</p>

<p>One can set an expiration when this URL no longer is valid.</p>

<p>With this method it is not possible to restrict e.g. the file size or
automatically know if the request was successful. This is possible with the
second option pre-signed POST.</p>

<h3>Pre-signed POST</h3>

<p>The second option that also supports browsers is to do an HTTP POST with some
pre-signed content that AWS S3 can verify to allow the upload. This option is
more advanced than a pre-signed URL and it is possible to control things much
more with this. It is also more complicated as it is a HTML form sent as a
multipart HTTP request.</p>

<p>The essence of this is that the owner (or someone with the right credentials)
generate a policy that exactly describe what is allowed and signs it so that
AWS S3 can verify that the HTTP POST is valid when it arrives.</p>

<p>Both expiration and file size limits are possible to set up with the policy [5].</p>

<p>One can also set up an HTTP redirect when something is successfully uploaded
with the given permissions. This is of course useful to close the loop not
knowing if someone actually uploaded what was intended and possibly leaving
dangling ends in the service.</p>

<h2>Summary</h2>

<p>Today there are several good option how to offload user content to ease the
workload on your endpoint. Options like Amazon S3 and Google Blob Store are
very good options. The essensce is to have a storage that is scalable and
restricted to what you need and then hand out permissions for read or write
as needed by the users of your platform. This allows you to focus on building
a great service and letting someone else deal with storing and delivering
static content to your users, globally.</p>

<h2>References</h2>

<ul>
<li>[1] <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html">Example Browser-based POST.</a></li>
<li>[2] <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-authentication-HTTPPOST.html">Authenticating requests in browser-based uploads using POST.</a></li>
<li>[3] <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/HTTPPOSTForms.html">HTML Forms</a></li>
<li>[4] <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html">Pre-Signed URL.</a></li>
<li>[5] <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html">Creating a POST policy.</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom Git Prompt]]></title>
    <link href="http://murrekatt.github.com/blog/2014/10/26/custom-git-prompt/"/>
    <updated>2014-10-26T12:08:00+01:00</updated>
    <id>http://murrekatt.github.com/blog/2014/10/26/custom-git-prompt</id>
    <content type="html"><![CDATA[<p>As an additional bonus to the previous
<a href="http://tommyback.com/blog/2014/10/24/git-workflow-overview/">post about Git workflows</a>
I&rsquo;ll show how to customize the shell prompt to give you useful information. I
have used <a href="http://bytebaker.com/2012/01/09/show-git-information-in-your-prompt/">this</a>
version as a starting-point with some additional fixes and improvements.</p>

<p>For me it&rsquo;s always useful to immediately see what branch I am on, if there
are local changes and how many commits I haven&rsquo;t yet pushed. To show all this
information in a compat way I have decided on the following:</p>

<pre><code>current_dir + ' ' + branch_name + [*] + ['(' + 3 + ')'] + ' $ '
</code></pre>

<p>First comes the current dir and then the branch name. Then optionally an asterix
for local uncommitted changes and lastly an optional number of unpushed commits.</p>

<p>Let&rsquo;s dive in and see how to build this step by step. Each part of the prompt
is got with a function &ndash; first our is the branch name.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">function </span>git-branch-name
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="nb">echo</span> <span class="k">$(</span>git branch | grep <span class="s2">&quot;^\*&quot;</span> | awk -F* <span class="o">{</span><span class="s1">&#39;print $NF&#39;</span><span class="o">}</span><span class="k">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The second piece is to check whether there are local changes that are not
committed.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">function </span>git-dirty <span class="o">{</span>
</span><span class='line'>    <span class="nv">st</span><span class="o">=</span><span class="k">$(</span>git status 2&gt;/dev/null | tail -1<span class="k">)</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$st</span> !<span class="o">=</span> <span class="s2">&quot;nothing to commit, working directory clean&quot;</span> <span class="o">]]</span> ; <span class="k">then</span>
</span><span class='line'><span class="k">        </span><span class="nb">echo</span> <span class="s2">&quot;*&quot;</span>
</span><span class='line'>    <span class="k">fi</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then let&rsquo;s see how many commits we are ahead locally and grab the number.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">function </span>git-unpushed <span class="o">{</span>
</span><span class='line'>    <span class="nv">brinfo</span><span class="o">=</span><span class="k">$(</span>git status | grep ahead<span class="k">)</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$brinfo</span> <span class="o">=</span>~ <span class="o">([</span>0-9<span class="o">]</span>+<span class="o">)[[</span>:space:<span class="o">]]</span>commit <span class="o">]]</span> ; <span class="k">then</span>
</span><span class='line'><span class="k">        </span><span class="nb">echo</span> <span class="s2">&quot;(${BASH_REMATCH[1]})&quot;</span>
</span><span class='line'>    <span class="k">fi</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The next part detects if we&rsquo;re in a git repo and then gitifies the prompt.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">function </span>gitify <span class="o">{</span>
</span><span class='line'>    git rev-parse --git-dir &gt; /dev/null 2&gt;&amp;1
</span><span class='line'>    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$?</span> !<span class="o">=</span> 0 <span class="o">]]</span> ; <span class="k">then</span>
</span><span class='line'><span class="k">        </span><span class="nb">echo</span> <span class="s2">&quot;&quot;</span>
</span><span class='line'>    <span class="k">else</span>
</span><span class='line'><span class="k">        </span><span class="nb">echo</span> <span class="k">$(</span>git-branch-name<span class="k">)$(</span>git-dirty<span class="k">)$(</span>git-unpushed<span class="k">)</span>
</span><span class='line'>    <span class="k">fi</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The last function constructs the prompt and add colors for nice visibility.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="k">function </span>make-prompt
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="nb">local </span><span class="nv">RED</span><span class="o">=</span><span class="s2">&quot;\[\e[0;31m\]&quot;</span>
</span><span class='line'>    <span class="nb">local </span><span class="nv">GREEN</span><span class="o">=</span><span class="s2">&quot;\[\e[0;32m\]&quot;</span>
</span><span class='line'>    <span class="nb">local </span><span class="nv">LIGHT_GRAY</span><span class="o">=</span><span class="s2">&quot;\[\e[0;37m\]&quot;</span>
</span><span class='line'>    <span class="nb">local </span><span class="nv">CYAN</span><span class="o">=</span><span class="s2">&quot;\[\e[0;36m\]&quot;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nv">PS1</span><span class="o">=</span><span class="s2">&quot;\</span>
</span><span class='line'><span class="s2">${GREEN} \W\</span>
</span><span class='line'><span class="s2">${RED} \$(gitify)\</span>
</span><span class='line'><span class="s2">${GREEN}\</span>
</span><span class='line'><span class="s2">${LIGHT_GRAY} $ &quot;</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'>make-prompt
</span></code></pre></td></tr></table></div></figure>


<p>I&rsquo;ve added all the above into a file called <code>.bash_prompt</code> that get sourced from
<code>.bashrc</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git Workflow Overview]]></title>
    <link href="http://murrekatt.github.com/blog/2014/10/24/git-workflow-overview/"/>
    <updated>2014-10-24T07:43:00+02:00</updated>
    <id>http://murrekatt.github.com/blog/2014/10/24/git-workflow-overview</id>
    <content type="html"><![CDATA[<p>Short overview of the most important git commands and workflows. This is mainly
focused on projects on Github.</p>

<h2>Forking, Cloning and Remotes</h2>

<p>Git allows you to work in several different ways. Two good ways to work are
direct push and pull requests. The former is often used when you are alone
or when you work on your own repo (which can be either a fork or not). The
latter is usually the way to work in an open-source project so that people
send pull requests for changes from their own forks of some repo.</p>

<p>To get started, let&rsquo;s get a local repo by cloning say your fork of another
repo you&rsquo;d like to contribute to.</p>

<pre><code>git clone &lt;repo_url&gt;
</code></pre>

<p>After this you have a local repo and a remote repo on Github. The repo repo
is called <code>origin</code>. This is where you push changes.</p>

<p>If you forked an upstream repo, you should also add a remote <code>upstream</code> so
that you can easily fetch and rebase.</p>

<pre><code>git remote add upstream &lt;upstream_repo_url&gt;
git fetch upstream
</code></pre>

<p>If you collaborate with others it might as well be good to add their repos
as remotes too so that you can grab changes from there and rebase on top of
changes not yet upstream.</p>

<h2>Making Changes</h2>

<p>Changes to the code base should be done in clear and well described steps
that are put into separate commits. I.e. a larger change will consist of a
number of commits describing various parts on the way towards the change is
completed.</p>

<p>A good workflow is to create a separate branch for some changes you&rsquo;re making.
This way you can easily work on multiple things in parallel and have good
isolation between things. To create a new branch you start from a commit hash
you want to use as the starting-point (usually this is HEAD of master).</p>

<pre><code>git checkout -b some_descriptive_name
</code></pre>

<p>Now you&rsquo;re in <code>some_descriptive_name</code> and it&rsquo;s identical to where you where
when you issued the checkout-branch command.</p>

<p>Now, let&rsquo;s say you edited a files <code>Example.cpp</code> and <code>Example.hpp</code> adding a new
function <code>get_number()</code>. Now you want to add the change into a commit and
this is done by adding the files like this:</p>

<pre><code>git add Example.cpp Example.hpp
</code></pre>

<p>After this add the change into a commit:</p>

<pre><code>git commit -m 'Added function get_number() into Example.'
</code></pre>

<p>This procedure is repeated for a collection of changes (commits) until you&rsquo;re
ready to send a pull request. Before you can send a pull request you need to
push your changes to your remote repo called <code>origin</code>:</p>

<pre><code>git push origin &lt;branch_name&gt;
</code></pre>

<h2>Creating a Pull Request</h2>

<p>When you have a set of changes that you&rsquo;d like to send upstream you do that by
sending a pull request. This means that you submit your branch and the included
changes to the upstream repo so that a maintainer can merge it.</p>

<p>Sending a pull request is done on Github from your own fork. Select the branch
you are on and then click create pull request.</p>

<p>Once this is created it will go through review by whoever is a maintainer of the
upstream repo.</p>

<h2>Rebasing a Pull Request</h2>

<p>One way of working is to rebase every pull request before it gets merged. This
results in a clean commit history. When this is the process, you&rsquo;ll need to
fetch any changes that were merged upstream and rebase on top of those.</p>

<pre><code>git fetch upstream
git rebase upstream/&lt;branch&gt;
</code></pre>

<p>This will catch up the current branch you are in with all changes upstream that
you fetched and then replay your local changes on top of that. If you want you
can automatically change pull to be fetch+rebase like this:</p>

<pre><code>git config --global branch.autosetuprebase always
</code></pre>

<p>and then retroactively add it to existing branches like this:</p>

<pre><code>git config branch.&lt;branch_name&gt;.rebase true
</code></pre>

<p>To update your pending pull request you need to update <code>origin</code> with your
rebased branch. To do this you need to force push</p>

<pre><code>git push +HEAD:&lt;branch&gt;
</code></pre>

<p>This will automatically update the pending pull request on Github to include
your updated commits.</p>

<p><strong>NOTE:</strong> force push rewrites history of your <code>origin</code> so be very careful with
using it to not lose commits.</p>

<h2>Resolving Conflicts when Rebasing</h2>

<p>Sometimes when you rebase there are changes that conflict with yours and then
git will stop and prompt you to make a choice what to do. In many cases there
is an automatic merge, but if this fails you need to resolve the conflicts
manually.</p>

<p>To do this you can start by looking at the files that have conflicts:</p>

<pre><code>git status
</code></pre>

<p>This lists files that you need to update. I.e. one by one you open the files
and look for e.g. HEAD to see where there&rsquo;s something you need to address. When
you have made the updates you need to add the files and continue with the
rebase like this:</p>

<pre><code>git add FileThatHadConflict.cpp
git rebase --continue
</code></pre>

<p>You might need to do this procedure multiple times depending on the conflicts
and number of commits you have etc.</p>

<h2>Interactive Rebase and Amending a Commit</h2>

<p>During review of your pull request there might be things that need to be
changed in a commit. There are two ways of dealing with this:</p>

<ul>
<li>add a new commit to change things.</li>
<li>amend an existing commit.</li>
</ul>


<p>The second option is cleaner, so let&rsquo;s look at this.</p>

<p>First you&rsquo;ll need to fetch latest upstream and rebase on that.</p>

<pre><code>git fetch upstream
git rebase upstream/&lt;branch&gt;
</code></pre>

<p>Then you need to do an interactive rebase so that you can mark the commits
you want to edit so that you can make changes directlyinto those.</p>

<pre><code>git rebase -i upstream/&lt;branch&gt;
</code></pre>

<p>To edit change <code>pick</code> to <code>e</code> for one or more commits. Save and exit. Git will
replay and stop at the marked commits so that you can make changes. So just
make the change and then amend and continue rebase like this:</p>

<pre><code>git add FileThatWasAmended.cpp
git commit --amend
# save commit info
git rebase --continue
</code></pre>

<p>Once you have done all you&rsquo;re ready to force push the changes to your pull
request.</p>

<pre><code>git push origin +HEAD:&lt;branch&gt;
</code></pre>

<p>If you for some reason what to abort the rebase you can do:</p>

<pre><code>git rebase --abort
</code></pre>

<h2>Rebasing On Top of Another Branch</h2>

<p>When you&rsquo;re collaborating with others it might many times be good to share
code before it hits upstream. This is easily done with git because upstream
is just like any other repo. For convenience, add a remote to the repo you
want to collaborate with like this:</p>

<pre><code>git remote add foo &lt;foo_repo_url&gt;
git fetch foo
</code></pre>

<p>Now you can see foo&rsquo;s branches with:</p>

<pre><code>git branch -v
</code></pre>

<p>And getting code from someone is as easy as fetching and rebasing on top of
upstream:</p>

<pre><code>git rebase foo/some_branch
</code></pre>

<p>The same is true for pull requests. You can send pull requests between forks
in Github too.</p>

<h2>Some Other Useful Commands</h2>

<p>To force HEAD to something in case of some major mistake you can do:</p>

<pre><code>git reset --hard &lt;commit_hash&gt;
</code></pre>

<p><strong>NOTE:</strong> be very careful with this command so you don&rsquo;t lose anything.</p>

<h2>Useful Links</h2>

<ul>
<li><a href="https://github.com/edx/edx-platform/wiki/How-to-Rebase-a-Pull-Request">How To Rebase a Pull Request</a></li>
<li><a href="http://mark-dot-net.blogspot.ch/2012/02/to-rebase-or-not-to-rebase-that-is.html">Rebase vs. Merge</a></li>
<li><a href="http://codeinthehole.com/writing/pull-requests-and-other-good-practices-for-teams-using-github/">Pull Requests and Other Good Practices</a></li>
<li><a href="http://sarah.thesharps.us/2014/09/01/the-gentle-art-of-patch-review/">Three Phase Review Process</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unit Testing: Part 4]]></title>
    <link href="http://murrekatt.github.com/blog/2013/10/30/unit-testing-part-4/"/>
    <updated>2013-10-30T18:22:00+01:00</updated>
    <id>http://murrekatt.github.com/blog/2013/10/30/unit-testing-part-4</id>
    <content type="html"><![CDATA[<p><em>In this post I will be showing something that is of major importance when designing software that can be tested. It is a very powerful technique to break dependencies and it&rsquo;s called Dependency Injection (DI). I hope you enjoy it!</em></p>

<h2>What is testable software?</h2>

<p>In order to build high quality software it helps a lot to have unit tests as a central part of the development process. In modern software development this is well understood and you can read more about this in the other parts I have written about unit testing.</p>

<p>Before it is possible to write unit tests, one must have code that can be tested. It is said that good software design and testable code go hand in hand. In my experience this is very true. What this means in practice is simply that well-designed code has the properties that are necessary to have in order to test it too. One of the key properties is to have code that has low dependencies and especially external dependencies like network, database or printer should not exist as direct dependencies to the code that we want to test. Hence, we want independent parts that are well-defined.</p>

<h2>So what is Dependency Injection?</h2>

<p>As the name suggests, it is a technique that injects the dependencies. Without a real context this probably sounds a bit weird, so imagine a class that uses a network connection. Instead of only having the network connection internally, it is probably better to give it when creating an instance of the class. I.e. inject the network connection or preferably inject an interface that can be implemented as a network connection. I wrote &ldquo;probably&rdquo; because it is of course not always correct to inject everything. This is no silver bullet either.</p>

<h2>Example in C++ with virtual interface</h2>

<p>Let&rsquo;s look at an example in C++ how it could look like when we break a dependency using DI. First, an example where a class uses a network connection internally to send some data.</p>

<pre><code>class Client
{
public:
  void send(const char* buffer, std::size_t length)
  {
    connection_.send(buffer, length);
  }

private:
  Connection connection_;
};
</code></pre>

<p>This is considered to be bad, because the class depends directly on the network connection itself. What we rather want is that the class is given an interface to something that we can implement as the network connection for production code, and mock in test. So, what would that look like?</p>

<p>First we need an interface. We define an abstract base class (ABC) that we can use internally in the class.</p>

<pre><code>struct IConnection
{
  virtual ~IConnection() {}
  virtual void send(const char* buffer, std::size_t length) = 0;
};
</code></pre>

<p>Next we need to modify the existing Connection class to implement the IConnection interface.</p>

<pre><code>class Connection : public IConnection
{
public:
  virtual ~Connection() {}
  virtual void send(const char* buffer, std::size_t length)
  {
    // ...
  }
};
</code></pre>

<p>Then we need to pass in this interface into the class in question and then use it as before.</p>

<pre><code>class Client
{
public:
  explicit Client(IConnection* connection) : connection_(connection) {}

  void send(const char* buffer, std::size_t length)
  {
    connection_-&gt;send(buffer, length);
  }

private:
  IConnection* connection_;
};
</code></pre>

<p>What we essentially have done is to instead of using a concrete class we now use an interface which means it can be exchanged for anything as long as it implements that interface. This is what we want, because then we can have one class that is the real network connection that we use for production code, and another class that we use in unit tests to not involve the network when testing the client itself. This is in line with good software design as well.</p>

<p>It is important to note that this is not always the wanted solution as there are other ways to achieve similar results. Let&rsquo;s look at an alternative.</p>

<h2>Example in C++ with a template</h2>

<p>Again we start with the same initial client that uses the network connection directly.</p>

<pre><code>class Client
{
public:
  void send(const char* buffer, std::size_t length)
  {
    connection_.send(buffer, length);
  }

private:
  Connection connection_;
};
</code></pre>

<p>Now instead we can change the class into a template so that we can pass in the connection.</p>

<pre><code>template &lt;typename TConnection&gt;
class Client
{
public:
  explicit Client(TConnection&amp; connection) : connection_(connection) {}

  void send(const char* buffer, std::size_t length)
  {
    connection_.send(buffer, length);
  }

private:
  TConnection&amp; connection_;
};
</code></pre>

<p>This achieves the same results if having a template is fine. In some cases, this might not be ideal and in other this is even better than having a virtual interface.</p>

<p>The same mechanism works for functions as well, i.e. one can break the dependency on a passed in argument by passing in an interface or making it into a template function an thus passing in a template argument.</p>

<h2>What is the bottom line?</h2>

<p>To me, DI is simply another tool that can be used when it makes sense. In my view, it does not make sense to inject everything just because it&rsquo;s possible. What does make sense to do every time is to write testable code and write unit tests to test it. This is part of the foundation of good quality.</p>

<p>DI is a concept that can be used across programming languages and here I have showed you how it can be used in C++. I hope you enjoyed the post!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unit Testing: Part 3]]></title>
    <link href="http://murrekatt.github.com/blog/2013/10/25/unit-testing-part-3/"/>
    <updated>2013-10-25T09:34:00+02:00</updated>
    <id>http://murrekatt.github.com/blog/2013/10/25/unit-testing-part-3</id>
    <content type="html"><![CDATA[<p><em>Another topic that is closely related to unit testing, or testing in general of course, is bug fixing. In this part I will describe what I consider important and how I like to work when dealing with bugs.</em></p>

<h2>How to approach a new bug?</h2>

<p>When confronted with a new bug the goal is often simplified to only finding a fix. This is of course important, but it&rsquo;s not enough. Equally important is to prevent the bug from happening again and also to not break anything else while fixing this one. To achieve the latter two, we need to rely on unit testing. First, the new bug needs to get unit tests that catch it so it becomes visible when it happens again. Second, to avoid breaking other things we need to have enough unit tests to be comfortable making changes to the code base so we can fix the bug we&rsquo;re working with.</p>

<h2>What is the best workflow?</h2>

<p>There are many ways one can work and below is a way I have found to work extremely well. It takes some discipline but it&rsquo;s a clear and straight-forward way.</p>

<h3>Reproduce the bug</h3>

<p>In order to properly understand and fix a bug one needs to reproduce it and see how the software fails. This is not always possible and in the worst case one cannot understand what the problem really is or what triggers it. Let&rsquo;s focus on the ones that can be reproduced and fixed.</p>

<h3>Use CI builds to track down when the bug was introduced</h3>

<p>Again, the Continuous Integration system can show its powers, because now we can use old builds to quickly find the first build that introduced this problem (assuming it&rsquo;s something that has been working at some point of course). If we find the last build where it works we can check what was changed in the following build looking at the change list. This I have found to be a very effective way of finding the introduction and to understand what parts of the code to look at. This basically means going back in time to when the bug was introduced.</p>

<h3>Add tests to catch the bug</h3>

<p>Once we can reproduce the bug we need to write one or more unit tests that trigger this bug. This is key considering future prevention of the same bug.</p>

<h3>Fix the bug and pass the tests</h3>

<p>Now we are set to fix the code so that the bug gets resolved and all the unit tests pass.</p>

<h3>Commit fix and verify that CI passes</h3>

<p>Lastly we commit the fix and make sure that the CI system passes.</p>

<p>This workflow is something I know works well and is concerned with programming and unit testing. It does not discard other types of testing or quality assurance that can come in addition to this later on.</p>

<h3>Next part</h3>

<p><em>In the next part I&rsquo;ll write about dependency injection which is a very powerful technique to break dependencies and thus get testable code. I hope you enjoyed this one!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unit Testing: Part 2]]></title>
    <link href="http://murrekatt.github.com/blog/2013/10/23/unit-testing-part-2/"/>
    <updated>2013-10-23T20:58:00+02:00</updated>
    <id>http://murrekatt.github.com/blog/2013/10/23/unit-testing-part-2</id>
    <content type="html"><![CDATA[<p><em>There are many related topics that come up when talking about unit testing. One of the most important ones in my mind is Continuous Integration. In this post I&rsquo;ll explain how I see Continuous Integration and its relationship with unit testing.</em></p>

<h2>What is Continuous Integration?</h2>

<p>Continuous Integration (CI) is a development process that runs whenever there are changes to be integrated into what one is building. The goal of CI in my view is to provide a platform to incrementally build high quality software and give transparency how changes affect things.</p>

<p>In the case of software, CI takes source code changes and then rebuilds the software to incorporate these. While incorporating the changes the process applies any defined quality assurance checks to see if the build passes. Essentially this means that one sets up the quality assurance mechanisms that are needed in order to catch issues that are relevant.</p>

<p>This is a key concept in modern software development and especially in Agile software development methodologies like Scrum.</p>

<h2>Where do unit tests fit in?</h2>

<p>Unit tests are part of the quality assurance mechanism and are run in order to make sure that the code changes are not breaking things. This is what unit testing automation is all about and this is a key ingredient of a good CI implementation.</p>

<p>In order to have a sound CI system with automated unit tests, there are a few key things to consider. Let&rsquo;s look at the ones I consider most important.</p>

<h3>There must be enough unit test coverage</h3>

<p>It&rsquo;s clear that one have to start from zero unit tests at some point and it&rsquo;s preferable if this is when there is zero source code as well. Adding unit tests after the fact is usually very slow and hard, so keep them with from the start. They are not only there to prove that things work but also to help while doing the development work.</p>

<h3>The unit tests must be fast</h3>

<p>One of the properties of a good unit test is that it runs very fast.</p>

<p>The goal is to have lots of unit tests because they increase the chances to catch bugs and thus increase software quality. Having many unit tests should not affect the time it takes to do a build very much, because we want to run all the tests for every build. It takes practice and skill to develop great software and the same goes for writing great unit tests.</p>

<h3>Changing code means adding and adjusting unit tests to cover the changes</h3>

<p>Working with unit tests are part of building software and therefore they are added, changed and removed as code is added, changed and removed. Very simple and takes discipline, especially when time is short.</p>

<h3>Failing unit tests means failing the build</h3>

<p>It is important to fail the build when there are failing unit tests. Even if there is only a single failing unit test. No compromise. Ever. This is part of being professional.</p>

<h2>Why is Continuous Integration so great?</h2>

<p>The main thing about CI is that it&rsquo;s a platform, a framework or simply a process that can take any shape one needs in order to get fast feedback to changes one is making while developing software. It sits in the venter of software development and can be hooked into other systems like source control and issue tracking systems to extend the automation of the software development process.</p>

<p>Put even simpler &ndash; it is the platform for automation which is a very powerful thing.</p>

<h3>Next part</h3>

<p><em>In the next part I&rsquo;ll write about bug fixing and unit tests. I hope you enjoyed this one!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unit Testing: Part 1]]></title>
    <link href="http://murrekatt.github.com/blog/2013/09/29/unit-testing-part-1/"/>
    <updated>2013-09-29T17:00:00+02:00</updated>
    <id>http://murrekatt.github.com/blog/2013/09/29/unit-testing-part-1</id>
    <content type="html"><![CDATA[<p><em>There exists lots of great information and books about unit testing and I&rsquo;m not trying to re-invent that or necessarily come with anything new. This is more of a distilled version preliminary for myself how I think and go about unit testing and what I have found to work well. If someone else finds it useful, even better!</em></p>

<h2>What is unit testing really?</h2>

<p>Unit testing is about writing code that tests e.g. classes and functions as you&rsquo;re developing software to check that things work as expected. Hence, this activity is <strong>something software engineers do themselves</strong> and not something other people do after code has been written! This realization is fundamental and important. Think of unit tests as a safety net for us programmers so that we don&rsquo;t break things. It&rsquo;s also important to understand that unit tests are for small things, generically called units. Unit tests are not aimed at testing functionality across several parts or features in a system. They&rsquo;re about making sure that the building blocks are delivering what they promise so one can integrate them into larger software systems. Other types of testing targets other testing needs such as functional-, integration- as well as performance testing. These are all important and it&rsquo;s crucial to understand and keep them separate.</p>

<p>Other important properties of unit tests are determinism, independency and speed. The first means that a test behaves in a deterministic manner and not random. Independency means that there are no external dependencies like database, network or disk. It&rsquo;s key to have very fast tests that are isolated and deterministic so that all tests can be run anytime you want, e.g. after any code change. Typically fast in this context means seconds to minutes and the number of tests means thousands or more.</p>

<p>Unit tests have more to them than only testing the integrity of the code, they also function as a specification of what something should do and how it should work. They show how to use specific classes and functions and this help newcomers as well as others to understand a code base or an API.</p>

<p>For unit tests to be valuable they must first of all be up to date and in use, but also descriptive, clean and focused. The added benefit means that when something breaks one will know precisely what broke because the tests are so clear and targeted. To achieve this one needs to be disciplined and have good practices and conventions.</p>

<h2>What does a good unit test look like?</h2>

<p>There are several ways to write good unit tests and here is what I have found to work very well for me in C++. Let&rsquo;s look at the following example test case.</p>

<pre><code>TEST(Client, successfully_connects)
{
  // Arrange
  Client client;
  // Act
  bool connected = client.connect();
  // Assert
  ASSERT_TRUE(connected);
}
</code></pre>

<p>Firstly, it&rsquo;s very important to read the test case clearly and immediately understand what it means and what to expect. My preferred way for test name is to write it in lowercase with underscores to delimit the words. This is much clearer than UpperCamelCase. It&rsquo;s important to name it according to what it&rsquo;s doing and what to expect so everyone clearly understands.</p>

<p>Secondly, I prefer to use <em>AAA</em>, that is, <em>Arrange</em>, <em>Act</em> and <em>Assert</em> to divide the test into clear parts. I don&rsquo;t aim to be dogmatic, but this works well most of the time. The first part arranges the necessary start (closely related to a fixture), the second part does the action, and the last part asserts what the test tests. This keeps the tests short, simple and clear. I find this providing a clear structure so anyone will understand the test and it restricts the tester to keep things short and focused. Many times I have seen single tests testing multiple things over multiple screens. This is not what you want! You want to have the user understand what the preconditions are i.e. the start and then understand what happens when you do &ldquo;one thing&rdquo;. That&rsquo;s it. Keep it simple is the key to success here as well.</p>

<h2>How to use fixtures to be DRY?</h2>

<p>To keep things clean and clear it&rsquo;s important to not clutter and repeat things. This is where fixtures enter the stage. I primarily think of a fixture as the preconditions that is the starting point for the test. They are about setting up state and tearing it down after the test.</p>

<p>For the previous test case example, we only have a starting point of the creating the client, so it doesn&rsquo;t give much immediate simplification, however, for larger number of tests this is very powerful. Let&rsquo;s take a look at what it would look like with a fixture.</p>

<pre><code>namespace
{
  struct ClientFixture
  {
    Client client_;
  };
}

TEST_F(ClientFixture, successfully_connects)
{
  // Arrange
  // Act
  bool connected = client_.connect();
  // Assert
  ASSERT_TRUE(connected);
}
</code></pre>

<p><em>As a side note, in C++ in .cpp files it&rsquo;s good practice to put code like the fixture into an anonymous namespace, because it should only be accessible in this compilation unit. If it&rsquo;s not put in an anonymous namespace it will cause linker errors if the same fixture name is also elsewhere. So, put it into the anonymous namespace.</em></p>

<p>With the fixture we can now easily add another test that uses the same fixture and thus saves us in this trivial example the code under arrange that created the client. This is now implicitly available through the fixture.</p>

<pre><code>TEST_F(ClientFixture, successfully_disconnects_even_if_not_connected)
{
  // Arrange
  bool connected = client_.isConnected();
  // Act
  bool disconnected = client_.disconnect();
  // Assert
  ASSERT_FALSE(connected);
  ASSERT_TRUE(disconnected);
}
</code></pre>

<h2>How to keep unit tests valuable and affordable?</h2>

<p>To keep unit tests valuable one needs to spend an effort to maintain their high quality. That means to refactor tests as the code they test change and to of course write new tests as new code is developed. All this comes at a price upfront, but pays itself back many times over the lifetime of a code base. This is key to understand in order to be successful long-term.</p>

<p>Another insight I have had is that one must take a pragmatic approach to what one tests and balance it with what needs to be done. One could of course spend close to an infinite amount of time writing tests and not get any working software to ship. That balance is got through experience in building software as well as understanding oneself and what is needed to produce quality software.</p>

<h3>Next part</h3>

<p><em>There are several related topics to unit testing like Continuous Integration, bug fixing, TDD and more. I&rsquo;ll cover these in later parts about unit testing. I hope you enjoyed it!</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Light-weight header only C++ unit testing framework]]></title>
    <link href="http://murrekatt.github.com/blog/2012/12/13/light-weight-header-only-c-plus-plus-unit-testing-framework/"/>
    <updated>2012-12-13T19:30:00+01:00</updated>
    <id>http://murrekatt.github.com/blog/2012/12/13/light-weight-header-only-c-plus-plus-unit-testing-framework</id>
    <content type="html"><![CDATA[<p>Unit testing is something I care about a lot and I&rsquo;ve come to realize the enormous benefits it brings when you want to refactor and make changes to software. This is something I do a lot, because I like to evolve things and thus have a basis where I can make changes and at the same time feel confident that it still works. Another very important aspect of unit tests is that you can communicate how something is expected to work, i.e. the specification, both to yourself and also to others. This is very important and powerful.</p>

<p>Currently, my main programming language is C++ and I&rsquo;m using <a href="https://code.google.com/p/googletest/">Google test</a> to write and run unit tests. This is a great framework and I&rsquo;m very happy with it. However, as an interesting exercise I decided to developed a small and light-weight unit testing framework that would work similarly to Google test, i.e. similar macros and so forth. My goal was to write a header-only test framework that has the most important features needed for unit testing. Also it should integrate with Continuous Integration systems like Hudson/Jenkins, i.e. the XML output must be readable by Hudson/Jenkins.</p>

<p>I&rsquo;ve completed an initial version I&rsquo;m using in a few toy projects and I thought it could be nice to share it in case someone else might find it useful. It&rsquo;s <a href="https://github.com/murrekatt/cpput">available for download on github</a>, so please have a look and let me know what you think.</p>

<p>In coming posts I&rsquo;ll describe how to use it and what my preferences when it comes to unit testing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First post]]></title>
    <link href="http://murrekatt.github.com/blog/2012/12/02/first-post/"/>
    <updated>2012-12-02T16:44:00+01:00</updated>
    <id>http://murrekatt.github.com/blog/2012/12/02/first-post</id>
    <content type="html"><![CDATA[<p>So, finally I&rsquo;ve decided to start collecting and sharing some things I&rsquo;ve come across that I think others might benefit from. Primarily it&rsquo;ll function as a notebook to myself about various topics. Hope you&rsquo;ll enjoy it!</p>
]]></content>
  </entry>
  
</feed>
